{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-17T09:00:55.728271Z","iopub.execute_input":"2023-10-17T09:00:55.728856Z","iopub.status.idle":"2023-10-17T09:00:56.113338Z","shell.execute_reply.started":"2023-10-17T09:00:55.728821Z","shell.execute_reply":"2023-10-17T09:00:56.111732Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:00:58.131923Z","iopub.execute_input":"2023-10-17T09:00:58.132670Z","iopub.status.idle":"2023-10-17T09:01:49.168386Z","shell.execute_reply.started":"2023-10-17T09:00:58.132633Z","shell.execute_reply":"2023-10-17T09:01:49.166995Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=0159eaf74bb76b5ccdeb70c4aeaec01399267f5755c7983a935ba5db79422d34\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=7><font color=\"orange\">***创建SparkDataFrame***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"markdown","source":"<font size=4>***3万字长文 PySpark入门级学习教程，框架思维: https://zhuanlan.zhihu.com/p/395431025***<font>","metadata":{}},{"cell_type":"markdown","source":"<font size=4>***1. 使用RDD来创建***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"code","source":"import os\nimport pyspark\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.types import StructType,StructField, StringType, IntegerType","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:01:49.273917Z","iopub.execute_input":"2023-10-17T09:01:49.274782Z","iopub.status.idle":"2023-10-17T09:01:49.284631Z","shell.execute_reply.started":"2023-10-17T09:01:49.274739Z","shell.execute_reply":"2023-10-17T09:01:49.283341Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"conf = SparkConf().setAppName(\"test_SamShare\").setMaster(\"local[4]\")\nsc = SparkContext(conf=conf)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:40:22.563739Z","iopub.execute_input":"2023-10-17T06:40:22.564149Z","iopub.status.idle":"2023-10-17T06:40:27.738757Z","shell.execute_reply.started":"2023-10-17T06:40:22.564116Z","shell.execute_reply":"2023-10-17T06:40:27.737523Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/17 06:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"rdd = sc.parallelize([(\"Sam\", 28, 88), (\"Flora\", 28, 90), (\"Run\", 1, 60)])\ndf = rdd.toDF([\"name\", \"age\", \"score\"])\ndf.show()\ndf.printSchema()\n\n# +-----+---+-----+\n# | name|age|score|\n# +-----+---+-----+\n# |  Sam| 28|   88|\n# |Flora| 28|   90|\n# |  Run|  1|   60|\n# +-----+---+-----+\n# root\n#  |-- name: string (nullable = true)\n#  |-- age: long (nullable = true)\n#  |-- score: long (nullable = true)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:41:07.690983Z","iopub.execute_input":"2023-10-17T06:41:07.691446Z","iopub.status.idle":"2023-10-17T06:41:07.737406Z","shell.execute_reply.started":"2023-10-17T06:41:07.691411Z","shell.execute_reply":"2023-10-17T06:41:07.735221Z"},"trusted":true},"execution_count":15,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m88\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlora\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m90\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m60\u001b[39m)])\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py:5239\u001b[0m, in \u001b[0;36mRDD.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m   5236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\n\u001b[1;32m   5237\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[Any]\u001b[39m\u001b[38;5;124m\"\u001b[39m, schema: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 5239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m   5240\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCALL_BEFORE_INITIALIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5241\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   5242\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD.toDF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5243\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5244\u001b[0m         },\n\u001b[1;32m   5245\u001b[0m     )\n","\u001b[0;31mPySparkRuntimeError\u001b[0m: [CALL_BEFORE_INITIALIZE] Not supported to call `RDD.toDF` before initialize SparkSession."],"ename":"PySparkRuntimeError","evalue":"[CALL_BEFORE_INITIALIZE] Not supported to call `RDD.toDF` before initialize SparkSession.","output_type":"error"}]},{"cell_type":"code","source":"data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\nrdd = spark.sparkContext.parallelize(data)\ndfFromRDD1 = rdd.toDF()\ndfFromRDD1.printSchema()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:44:09.070808Z","iopub.execute_input":"2023-10-17T06:44:09.071252Z","iopub.status.idle":"2023-10-17T06:44:11.419962Z","shell.execute_reply.started":"2023-10-17T06:44:09.071221Z","shell.execute_reply":"2023-10-17T06:44:11.419084Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"root\n |-- _1: string (nullable = true)\n |-- _2: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***2.使用python的DataFrame来创建***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"code","source":"df = pd.DataFrame([['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]],\n                  columns=['name', 'age', 'score'])\nprint(\">> 打印DataFrame:\")\nprint(df)\nprint(\"\\n\")\nSpark_df = spark.createDataFrame(df)\nprint(\">> 打印SparkDataFrame:\")\nSpark_df.show()\n# >> 打印DataFrame:\n#     name  age  score\n# 0    Sam   28     88\n# 1  Flora   28     90\n# 2    Run    1     60\n# >> 打印SparkDataFrame:\n# +-----+---+-----+\n# | name|age|score|\n# +-----+---+-----+\n# |  Sam| 28|   88|\n# |Flora| 28|   90|\n# |  Run|  1|   60|\n# +-----+---+-----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:45:27.086269Z","iopub.execute_input":"2023-10-17T06:45:27.086911Z","iopub.status.idle":"2023-10-17T06:45:29.794690Z","shell.execute_reply.started":"2023-10-17T06:45:27.086862Z","shell.execute_reply":"2023-10-17T06:45:29.793930Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":">> 打印DataFrame:\n    name  age  score\n0    Sam   28     88\n1  Flora   28     90\n2    Run    1     60\n\n\n>> 打印SparkDataFrame:\n+-----+---+-----+\n| name|age|score|\n+-----+---+-----+\n|  Sam| 28|   88|\n|Flora| 28|   90|\n|  Run|  1|   60|\n+-----+---+-----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***3.使用List来创建***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"code","source":"list_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]]\nSpark_df = spark.createDataFrame(list_values, ['name', 'age', 'score'])\nSpark_df.show()\n# +-----+---+-----+\n# | name|age|score|\n# +-----+---+-----+\n# |  Sam| 28|   88|\n# |Flora| 28|   90|\n# |  Run|  1|   60|\n# +-----+---+-----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:46:39.071610Z","iopub.execute_input":"2023-10-17T06:46:39.072026Z","iopub.status.idle":"2023-10-17T06:46:39.521739Z","shell.execute_reply.started":"2023-10-17T06:46:39.071948Z","shell.execute_reply":"2023-10-17T06:46:39.520929Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"+-----+---+-----+\n| name|age|score|\n+-----+---+-----+\n|  Sam| 28|   88|\n|Flora| 28|   90|\n|  Run|  1|   60|\n+-----+---+-----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***4. 读取数据文件来创建***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"code","source":"# 4.1 CSV文件\ndf = spark.read.option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .option(\"delimiter\", \",\")\\\n    .csv(\"./test/data/titanic/train.csv\")\ndf.show(5)\ndf.printSchema()\n\n# 4.2 json文件\ndf = spark.read.json(\"./test/data/hello_samshare.json\")\ndf.show(5)\ndf.printSchema()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=4>***5. 通过读取数据库来创建***<font>","metadata":{}},{"cell_type":"code","source":"# 5.1 读取hive数据\nspark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\nspark.sql(\"LOAD DATA LOCAL INPATH 'data/kv1.txt' INTO TABLE src\")\ndf = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\ndf.show(5)\n\n# 5.2 读取mysql数据\nurl = \"jdbc:mysql://localhost:3306/test\"\ndf = spark.read.format(\"jdbc\") \\\n .option(\"url\", url) \\\n .option(\"dbtable\", \"runoob_tbl\") \\\n .option(\"user\", \"root\") \\\n .option(\"password\", \"8888\") \\\n .load()\ndf.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:48:04.494353Z","iopub.execute_input":"2023-10-17T06:48:04.494658Z","iopub.status.idle":"2023-10-17T06:48:05.074264Z","shell.execute_reply.started":"2023-10-17T06:48:04.494633Z","shell.execute_reply":"2023-10-17T06:48:05.073122Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 5.1 读取hive数据\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOAD DATA LOCAL INPATH \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/kv1.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m INTO TABLE src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT key, value FROM src WHERE key < 10 ORDER BY key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`src`, Ignore\n"],"ename":"AnalysisException","evalue":"[NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n'CreateTable `spark_catalog`.`default`.`src`, Ignore\n","output_type":"error"}]},{"cell_type":"markdown","source":"<font size=7><font color=\"orange\">***常用的SparkDataFrame API***<font>","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import functions as F\nfrom pyspark.sql import SparkSession\n# SparkSQL的许多功能封装在SparkSession的方法接口中, SparkContext则不行的。\nspark = SparkSession.builder \\\n    .appName(\"sam_SamShare\") \\\n    .config(\"master\", \"local[4]\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\nsc = spark.sparkContext\n\n# 创建一个SparkDataFrame\nrdd = sc.parallelize([(\"Sam\", 28, 88, \"M\"),\n                      (\"Flora\", 28, 90, \"F\"),\n                      (\"Run\", 1, 60, None),\n                      (\"Peter\", 55, 100, \"M\"),\n                      (\"Mei\", 54, 95, \"F\")])\ndf = rdd.toDF([\"name\", \"age\", \"score\", \"sex\"])\ndf.show()\ndf.printSchema()\n\n# +-----+---+-----+----+\n# | name|age|score| sex|\n# +-----+---+-----+----+\n# |  Sam| 28|   88|   M|\n# |Flora| 28|   90|   F|\n# |  Run|  1|   60|null|\n# |Peter| 55|  100|   M|\n# |  Mei| 54|   95|   F|\n# +-----+---+-----+----+\n# root\n#  |-- name: string (nullable = true)\n#  |-- age: long (nullable = true)\n#  |-- score: long (nullable = true)\n#  |-- sex: string (nullable = true)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:48:41.563104Z","iopub.execute_input":"2023-10-17T06:48:41.563442Z","iopub.status.idle":"2023-10-17T06:48:42.145282Z","shell.execute_reply.started":"2023-10-17T06:48:41.563419Z","shell.execute_reply":"2023-10-17T06:48:42.144344Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"23/10/17 06:48:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+-----+---+-----+----+\n| name|age|score| sex|\n+-----+---+-----+----+\n|  Sam| 28|   88|   M|\n|Flora| 28|   90|   F|\n|  Run|  1|   60|NULL|\n|Peter| 55|  100|   M|\n|  Mei| 54|   95|   F|\n+-----+---+-----+----+\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- score: long (nullable = true)\n |-- sex: string (nullable = true)\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***1. 查看DataFrame的APIs***<font>","metadata":{}},{"cell_type":"code","source":"# DataFrame.collect\n# 以列表形式返回行\ndf.collect()\n# [Row(name='Sam', age=28, score=88, sex='M'),\n# Row(name='Flora', age=28, score=90, sex='F'),\n# Row(name='Run', age=1, score=60, sex=None),\n# Row(name='Peter', age=55, score=100, sex='M'),\n# Row(name='Mei', age=54, score=95, sex='F')]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:20.274974Z","iopub.execute_input":"2023-10-17T06:50:20.275337Z","iopub.status.idle":"2023-10-17T06:50:20.724418Z","shell.execute_reply.started":"2023-10-17T06:50:20.275314Z","shell.execute_reply":"2023-10-17T06:50:20.723189Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[Row(name='Sam', age=28, score=88, sex='M'),\n Row(name='Flora', age=28, score=90, sex='F'),\n Row(name='Run', age=1, score=60, sex=None),\n Row(name='Peter', age=55, score=100, sex='M'),\n Row(name='Mei', age=54, score=95, sex='F')]"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.count\ndf.count()\n# 5","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:27.609504Z","iopub.execute_input":"2023-10-17T06:50:27.609942Z","iopub.status.idle":"2023-10-17T06:50:28.414983Z","shell.execute_reply.started":"2023-10-17T06:50:27.609908Z","shell.execute_reply":"2023-10-17T06:50:28.414058Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.columns\ndf.columns\n# ['name', 'age', 'score', 'sex']","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:33.984706Z","iopub.execute_input":"2023-10-17T06:50:33.985065Z","iopub.status.idle":"2023-10-17T06:50:33.992727Z","shell.execute_reply.started":"2023-10-17T06:50:33.985037Z","shell.execute_reply":"2023-10-17T06:50:33.991534Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"['name', 'age', 'score', 'sex']"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.dtypes\ndf.dtypes\n# [('name', 'string'), ('age', 'bigint'), ('score', 'bigint'), ('sex', 'string')]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:39.565824Z","iopub.execute_input":"2023-10-17T06:50:39.567519Z","iopub.status.idle":"2023-10-17T06:50:39.573420Z","shell.execute_reply.started":"2023-10-17T06:50:39.567474Z","shell.execute_reply":"2023-10-17T06:50:39.572363Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[('name', 'string'), ('age', 'bigint'), ('score', 'bigint'), ('sex', 'string')]"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.describe\n# 返回列的基础统计信息\ndf.describe(['age']).show()\n# +-------+------------------+\n# |summary|               age|\n# +-------+------------------+\n# |  count|                 5|\n# |   mean|              33.2|\n# | stddev|22.353970564532826|\n# |    min|                 1|\n# |    max|                55|\n# +-------+------------------+\ndf.describe().show()\n# +-------+-----+------------------+------------------+----+\n# |summary| name|               age|             score| sex|\n# +-------+-----+------------------+------------------+----+\n# |  count|    5|                 5|                 5|   4|\n# |   mean| null|              33.2|              86.6|null|\n# | stddev| null|22.353970564532826|15.582040944625966|null|\n# |    min|Flora|                 1|                60|   F|\n# |    max|  Sam|                55|               100|   M|\n# +-------+-----+------------------+------------------+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:46.435473Z","iopub.execute_input":"2023-10-17T06:50:46.435829Z","iopub.status.idle":"2023-10-17T06:50:48.369370Z","shell.execute_reply.started":"2023-10-17T06:50:46.435798Z","shell.execute_reply":"2023-10-17T06:50:48.367990Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"+-------+------------------+\n|summary|               age|\n+-------+------------------+\n|  count|                 5|\n|   mean|              33.2|\n| stddev|22.353970564532826|\n|    min|                 1|\n|    max|                55|\n+-------+------------------+\n\n","output_type":"stream"},{"name":"stderr","text":"23/10/17 06:50:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 21:>                                                         (0 + 4) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------+-----+------------------+------------------+----+\n|summary| name|               age|             score| sex|\n+-------+-----+------------------+------------------+----+\n|  count|    5|                 5|                 5|   4|\n|   mean| NULL|              33.2|              86.6|NULL|\n| stddev| NULL|22.353970564532826|15.582040944625966|NULL|\n|    min|Flora|                 1|                60|   F|\n|    max|  Sam|                55|               100|   M|\n+-------+-----+------------------+------------------+----+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.select\n# 选定指定列并按照一定顺序呈现\ndf.select(\"sex\", \"score\").show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:50:59.215839Z","iopub.execute_input":"2023-10-17T06:50:59.216523Z","iopub.status.idle":"2023-10-17T06:50:59.732035Z","shell.execute_reply.started":"2023-10-17T06:50:59.216468Z","shell.execute_reply":"2023-10-17T06:50:59.730999Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"+----+-----+\n| sex|score|\n+----+-----+\n|   M|   88|\n|   F|   90|\n|NULL|   60|\n|   M|  100|\n|   F|   95|\n+----+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.first\n# DataFrame.head\n# 查看第1条数据\ndf.first()\n# Row(name='Sam', age=28, score=88, sex='M')\ndf.head(1)\n# [Row(name='Sam', age=28, score=88, sex='M')]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:51:07.101133Z","iopub.execute_input":"2023-10-17T06:51:07.101506Z","iopub.status.idle":"2023-10-17T06:51:07.368785Z","shell.execute_reply.started":"2023-10-17T06:51:07.101474Z","shell.execute_reply":"2023-10-17T06:51:07.367279Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[Row(name='Sam', age=28, score=88, sex='M')]"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.freqItems\n# 查看指定列的枚举值\ndf.freqItems([\"age\",\"sex\"]).show()\n# +---------------+-------------+\n# |  age_freqItems|sex_freqItems|\n# +---------------+-------------+\n# |[55, 1, 28, 54]|      [M, F,]|\n# +---------------+-------------+\n","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:51:19.731714Z","iopub.execute_input":"2023-10-17T06:51:19.732032Z","iopub.status.idle":"2023-10-17T06:51:20.255266Z","shell.execute_reply.started":"2023-10-17T06:51:19.732008Z","shell.execute_reply":"2023-10-17T06:51:20.254475Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"+---------------+-------------+\n|  age_freqItems|sex_freqItems|\n+---------------+-------------+\n|[55, 1, 28, 54]| [M, NULL, F]|\n+---------------+-------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.summary\ndf.summary().show()\n# +-------+-----+------------------+------------------+----+\n# |summary| name|               age|             score| sex|\n# +-------+-----+------------------+------------------+----+\n# |  count|    5|                 5|                 5|   4|\n# |   mean| null|              33.2|              86.6|null|\n# | stddev| null|22.353970564532826|15.582040944625966|null|\n# |    min|Flora|                 1|                60|   F|\n# |    25%| null|                28|                88|null|\n# |    50%| null|                28|                90|null|\n# |    75%| null|                54|                95|null|\n# |    max|  Sam|                55|               100|   M|\n# +-------+-----+------------------+------------------+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:51:29.430233Z","iopub.execute_input":"2023-10-17T06:51:29.430594Z","iopub.status.idle":"2023-10-17T06:51:30.379278Z","shell.execute_reply.started":"2023-10-17T06:51:29.430566Z","shell.execute_reply":"2023-10-17T06:51:30.378506Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"+-------+-----+------------------+------------------+----+\n|summary| name|               age|             score| sex|\n+-------+-----+------------------+------------------+----+\n|  count|    5|                 5|                 5|   4|\n|   mean| NULL|              33.2|              86.6|NULL|\n| stddev| NULL|22.353970564532826|15.582040944625966|NULL|\n|    min|Flora|                 1|                60|   F|\n|    25%| NULL|                28|                88|NULL|\n|    50%| NULL|                28|                90|NULL|\n|    75%| NULL|                54|                95|NULL|\n|    max|  Sam|                55|               100|   M|\n+-------+-----+------------------+------------------+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.sample\n# 按照一定规则从df随机抽样数据\ndf.sample(0.5).show()\n# +-----+---+-----+----+\n# | name|age|score| sex|\n# +-----+---+-----+----+\n# |  Sam| 28|   88|   M|\n# |  Run|  1|   60|null|\n# |Peter| 55|  100|   M|\n# +-----+---+-----+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:51:36.181205Z","iopub.execute_input":"2023-10-17T06:51:36.181608Z","iopub.status.idle":"2023-10-17T06:51:36.657845Z","shell.execute_reply.started":"2023-10-17T06:51:36.181575Z","shell.execute_reply":"2023-10-17T06:51:36.656972Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"+----+---+-----+----+\n|name|age|score| sex|\n+----+---+-----+----+\n| Run|  1|   60|NULL|\n| Mei| 54|   95|   F|\n+----+---+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***2. 简单处理DataFrame的APIs***<font>","metadata":{}},{"cell_type":"code","source":"# DataFrame.distinct\n# 对数据集进行去重\ndf.distinct().show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:52:01.700724Z","iopub.execute_input":"2023-10-17T06:52:01.701137Z","iopub.status.idle":"2023-10-17T06:52:02.324826Z","shell.execute_reply.started":"2023-10-17T06:52:01.701107Z","shell.execute_reply":"2023-10-17T06:52:02.324174Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"+-----+---+-----+----+\n| name|age|score| sex|\n+-----+---+-----+----+\n|  Sam| 28|   88|   M|\n|Flora| 28|   90|   F|\n|  Mei| 54|   95|   F|\n|Peter| 55|  100|   M|\n|  Run|  1|   60|NULL|\n+-----+---+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.dropDuplicates\n# 对指定列去重\ndf.dropDuplicates([\"sex\"]).show()\n# +-----+---+-----+----+\n# | name|age|score| sex|\n# +-----+---+-----+----+\n# |Flora| 28|   90|   F|\n# |  Run|  1|   60|null|\n# |  Sam| 28|   88|   M|\n# +-----+---+-----+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:52:09.242711Z","iopub.execute_input":"2023-10-17T06:52:09.243052Z","iopub.status.idle":"2023-10-17T06:52:09.964557Z","shell.execute_reply.started":"2023-10-17T06:52:09.243025Z","shell.execute_reply":"2023-10-17T06:52:09.963468Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"+-----+---+-----+----+\n| name|age|score| sex|\n+-----+---+-----+----+\n|  Run|  1|   60|NULL|\n|Flora| 28|   90|   F|\n|  Sam| 28|   88|   M|\n+-----+---+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.exceptAll\n# DataFrame.subtract\n# 根据指定的df对df进行去重\ndf1 = spark.createDataFrame(\n        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\ndf2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\ndf3 = df1.exceptAll(df2)  # 没有去重的功效\ndf4 = df1.subtract(df2)  # 有去重的奇效\ndf1.show()\ndf2.show()\ndf3.show()\ndf4.show()\n# +---+---+\n# | C1| C2|\n# +---+---+\n# |  a|  1|\n# |  a|  1|\n# |  b|  3|\n# |  c|  4|\n# +---+---+\n# +---+---+\n# | C1| C2|\n# +---+---+\n# |  a|  1|\n# |  b|  3|\n# +---+---+\n# +---+---+\n# | C1| C2|\n# +---+---+\n# |  a|  1|\n# |  c|  4|\n# +---+---+\n# +---+---+\n# | C1| C2|\n# +---+---+\n# |  c|  4|\n# +---+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:52:21.101936Z","iopub.execute_input":"2023-10-17T06:52:21.102303Z","iopub.status.idle":"2023-10-17T06:52:23.967812Z","shell.execute_reply.started":"2023-10-17T06:52:21.102276Z","shell.execute_reply":"2023-10-17T06:52:23.966956Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  a|  1|\n|  b|  3|\n|  c|  4|\n+---+---+\n\n+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  b|  3|\n+---+---+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n|  c|  4|\n+---+---+\n\n+---+---+\n| C1| C2|\n+---+---+\n|  c|  4|\n+---+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.intersectAll\n# 返回两个DataFrame的交集\ndf1 = spark.createDataFrame(\n        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\ndf2 = spark.createDataFrame([(\"a\", 1), (\"b\", 4)], [\"C1\", \"C2\"])\ndf1.intersectAll(df2).show()\n# +---+---+\n# | C1| C2|\n# +---+---+\n# |  a|  1|\n# +---+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:52:55.064458Z","iopub.execute_input":"2023-10-17T06:52:55.064811Z","iopub.status.idle":"2023-10-17T06:52:55.922764Z","shell.execute_reply.started":"2023-10-17T06:52:55.064788Z","shell.execute_reply":"2023-10-17T06:52:55.920799Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"+---+---+\n| C1| C2|\n+---+---+\n|  a|  1|\n+---+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.drop\n# 丢弃指定列\ndf.drop('age').show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:53:26.541569Z","iopub.execute_input":"2023-10-17T06:53:26.541913Z","iopub.status.idle":"2023-10-17T06:53:26.891689Z","shell.execute_reply.started":"2023-10-17T06:53:26.541870Z","shell.execute_reply":"2023-10-17T06:53:26.890947Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"+-----+-----+----+\n| name|score| sex|\n+-----+-----+----+\n|  Sam|   88|   M|\n|Flora|   90|   F|\n|  Run|   60|NULL|\n|Peter|  100|   M|\n|  Mei|   95|   F|\n+-----+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.withColumn\n# 新增列\ndf1 = df.withColumn(\"birth_year\", 2021 - df.age)\ndf1.show()\n# +-----+---+-----+----+----------+\n# | name|age|score| sex|birth_year|\n# +-----+---+-----+----+----------+\n# |  Sam| 28|   88|   M|      1993|\n# |Flora| 28|   90|   F|      1993|\n# |  Run|  1|   60|null|      2020|\n# |Peter| 55|  100|   M|      1966|\n# |  Mei| 54|   95|   F|      1967|\n# +-----+---+-----+----+----------+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:53:40.206011Z","iopub.execute_input":"2023-10-17T06:53:40.206355Z","iopub.status.idle":"2023-10-17T06:53:40.596442Z","shell.execute_reply.started":"2023-10-17T06:53:40.206328Z","shell.execute_reply":"2023-10-17T06:53:40.595675Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"+-----+---+-----+----+----------+\n| name|age|score| sex|birth_year|\n+-----+---+-----+----+----------+\n|  Sam| 28|   88|   M|      1993|\n|Flora| 28|   90|   F|      1993|\n|  Run|  1|   60|NULL|      2020|\n|Peter| 55|  100|   M|      1966|\n|  Mei| 54|   95|   F|      1967|\n+-----+---+-----+----+----------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.withColumnRenamed\n# 重命名列名\ndf1 = df.withColumnRenamed(\"sex\", \"gender\")\ndf1.show()\n# +-----+---+-----+------+\n# | name|age|score|gender|\n# +-----+---+-----+------+\n# |  Sam| 28|   88|     M|\n# |Flora| 28|   90|     F|\n# |  Run|  1|   60|  null|\n# |Peter| 55|  100|     M|\n# |  Mei| 54|   95|     F|\n# +-----+---+-----+------+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:53:48.038851Z","iopub.execute_input":"2023-10-17T06:53:48.039200Z","iopub.status.idle":"2023-10-17T06:53:48.412110Z","shell.execute_reply.started":"2023-10-17T06:53:48.039176Z","shell.execute_reply":"2023-10-17T06:53:48.410980Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"+-----+---+-----+------+\n| name|age|score|gender|\n+-----+---+-----+------+\n|  Sam| 28|   88|     M|\n|Flora| 28|   90|     F|\n|  Run|  1|   60|  NULL|\n|Peter| 55|  100|     M|\n|  Mei| 54|   95|     F|\n+-----+---+-----+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.dropna\n# 丢弃空值，DataFrame.dropna(how='any', thresh=None, subset=None)\ndf.dropna(how='all', subset=['sex']).show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |  Sam| 28|   88|  M|\n# |Flora| 28|   90|  F|\n# |Peter| 55|  100|  M|\n# |  Mei| 54|   95|  F|\n# +-----+---+-----+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:53:55.182268Z","iopub.execute_input":"2023-10-17T06:53:55.182602Z","iopub.status.idle":"2023-10-17T06:53:55.571221Z","shell.execute_reply.started":"2023-10-17T06:53:55.182572Z","shell.execute_reply":"2023-10-17T06:53:55.570286Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|  Sam| 28|   88|  M|\n|Flora| 28|   90|  F|\n|Peter| 55|  100|  M|\n|  Mei| 54|   95|  F|\n+-----+---+-----+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.fillna\n# 空值填充操作\ndf1 = spark.createDataFrame(\n        [(\"a\", None), (\"a\", 1), (None,  3), (\"c\", 4)], [\"C1\", \"C2\"])\n# df2 = df1.na.fill({\"C1\": \"d\", \"C2\": 99})\ndf2 = df1.fillna({\"C1\": \"d\", \"C2\": 99})\ndf1.show()\ndf2.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:54:06.322809Z","iopub.execute_input":"2023-10-17T06:54:06.323170Z","iopub.status.idle":"2023-10-17T06:54:06.961400Z","shell.execute_reply.started":"2023-10-17T06:54:06.323149Z","shell.execute_reply":"2023-10-17T06:54:06.960337Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"+----+----+\n|  C1|  C2|\n+----+----+\n|   a|NULL|\n|   a|   1|\n|NULL|   3|\n|   c|   4|\n+----+----+\n\n+---+---+\n| C1| C2|\n+---+---+\n|  a| 99|\n|  a|  1|\n|  d|  3|\n|  c|  4|\n+---+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.filter\n# 根据条件过滤\ndf.filter(df.age>50).show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |Peter| 55|  100|  M|\n# |  Mei| 54|   95|  F|\n# +-----+---+-----+---+\ndf.where(df.age==28).show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |  Sam| 28|   88|  M|\n# |Flora| 28|   90|  F|\n# +-----+---+-----+---+\ndf.filter(\"age<18\").show()\n# +----+---+-----+----+\n# |name|age|score| sex|\n# +----+---+-----+----+\n# | Run|  1|   60|null|\n# +----+---+-----+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:54:21.700028Z","iopub.execute_input":"2023-10-17T06:54:21.700361Z","iopub.status.idle":"2023-10-17T06:54:22.946494Z","shell.execute_reply.started":"2023-10-17T06:54:21.700340Z","shell.execute_reply":"2023-10-17T06:54:22.945553Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|Peter| 55|  100|  M|\n|  Mei| 54|   95|  F|\n+-----+---+-----+---+\n\n+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|  Sam| 28|   88|  M|\n|Flora| 28|   90|  F|\n+-----+---+-----+---+\n\n+----+---+-----+----+\n|name|age|score| sex|\n+----+---+-----+----+\n| Run|  1|   60|NULL|\n+----+---+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.join\n# 这个不用多解释了，直接上案例来看看具体的语法即可，DataFrame.join(other, on=None, how=None)\ndf1 = spark.createDataFrame(\n        [(\"a\", 1), (\"d\", 1), (\"b\",  3), (\"c\", 4)], [\"id\", \"num1\"])\ndf2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num2\"])\ndf1.join(df2, df1.id == df2.id, 'left').select(df1.id.alias(\"df1_id\"),\n                                               df1.num1.alias(\"df1_num\"),\n                                               df2.num2.alias(\"df2_num\")\n                                               ).sort([\"df1_id\"], ascending=False)\\\n    .show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:54:43.991422Z","iopub.execute_input":"2023-10-17T06:54:43.991771Z","iopub.status.idle":"2023-10-17T06:54:45.305101Z","shell.execute_reply.started":"2023-10-17T06:54:43.991742Z","shell.execute_reply":"2023-10-17T06:54:45.304296Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"[Stage 85:=============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+------+-------+-------+\n|df1_id|df1_num|df2_num|\n+------+-------+-------+\n|     d|      1|   NULL|\n|     c|      4|   NULL|\n|     b|      3|      3|\n|     a|      1|      1|\n+------+-------+-------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.agg(*exprs)\n# 聚合数据，可以写多个聚合方法，如果不写groupBy的话就是对整个DF进行聚合\n# DataFrame.alias\n# 设置列或者DataFrame别名\n# DataFrame.groupBy\n# 根据某几列进行聚合，如有多列用列表写在一起，如 df.groupBy([\"sex\", \"age\"])\ndf.groupBy(\"sex\").agg(F.min(df.age).alias(\"最小年龄\"),\n                      F.expr(\"avg(age)\").alias(\"平均年龄\"),\n                      F.expr(\"collect_list(name)\").alias(\"姓名集合\")\n                      ).show()\n# +----+--------+--------+------------+\n# | sex|最小年龄|平均年龄|    姓名集合|\n# +----+--------+--------+------------+\n# |   F|      28|    41.0|[Flora, Mei]|\n# |null|       1|     1.0|       [Run]|\n# |   M|      28|    41.5|[Sam, Peter]|\n# +----+--------+--------+------------+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:55:08.617518Z","iopub.execute_input":"2023-10-17T06:55:08.617962Z","iopub.status.idle":"2023-10-17T06:55:09.301814Z","shell.execute_reply.started":"2023-10-17T06:55:08.617924Z","shell.execute_reply":"2023-10-17T06:55:09.300612Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"+----+--------+--------+------------+\n| sex|最小年龄|平均年龄|    姓名集合|\n+----+--------+--------+------------+\n|   M|      28|    41.5|[Sam, Peter]|\n|   F|      28|    41.0|[Flora, Mei]|\n|NULL|       1|     1.0|       [Run]|\n+----+--------+--------+------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.foreach\n# 对每一行进行函数方法的应用\ndef f(person):\n    print(person.name)\ndf.foreach(f)\n# Peter\n# Run\n# Sam\n# Flora\n# Mei","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:55:28.952299Z","iopub.execute_input":"2023-10-17T06:55:28.952738Z","iopub.status.idle":"2023-10-17T06:55:29.461532Z","shell.execute_reply.started":"2023-10-17T06:55:28.952702Z","shell.execute_reply":"2023-10-17T06:55:29.460762Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Sam\nPeter\nMei\nRun\nFlora\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.replace\n# 修改df里的某些值\ndf1 = df.na.replace({\"M\": \"Male\", \"F\": \"Female\"})\ndf1.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:55:47.116665Z","iopub.execute_input":"2023-10-17T06:55:47.117008Z","iopub.status.idle":"2023-10-17T06:55:47.475802Z","shell.execute_reply.started":"2023-10-17T06:55:47.116980Z","shell.execute_reply":"2023-10-17T06:55:47.474472Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"+-----+---+-----+------+\n| name|age|score|   sex|\n+-----+---+-----+------+\n|  Sam| 28|   88|  Male|\n|Flora| 28|   90|Female|\n|  Run|  1|   60|  NULL|\n|Peter| 55|  100|  Male|\n|  Mei| 54|   95|Female|\n+-----+---+-----+------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.union\n# 相当于SQL里的union all操作\ndf1 = spark.createDataFrame(\n        [(\"a\", 1), (\"d\", 1), (\"b\",  3), (\"c\", 4)], [\"id\", \"num\"])\ndf2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num\"])\ndf1.union(df2).show()\ndf1.unionAll(df2).show()\n# 这里union没有去重，不知道为啥，有知道的朋友麻烦解释下，谢谢了。\n# +---+---+\n# | id|num|\n# +---+---+\n# |  a|  1|\n# |  d|  1|\n# |  b|  3|\n# |  c|  4|\n# |  a|  1|\n# |  b|  3|\n# +---+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:56:04.568126Z","iopub.execute_input":"2023-10-17T06:56:04.568571Z","iopub.status.idle":"2023-10-17T06:56:06.038971Z","shell.execute_reply.started":"2023-10-17T06:56:04.568532Z","shell.execute_reply":"2023-10-17T06:56:06.038166Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"+---+---+\n| id|num|\n+---+---+\n|  a|  1|\n|  d|  1|\n|  b|  3|\n|  c|  4|\n|  a|  1|\n|  b|  3|\n+---+---+\n\n+---+---+\n| id|num|\n+---+---+\n|  a|  1|\n|  d|  1|\n|  b|  3|\n|  c|  4|\n|  a|  1|\n|  b|  3|\n+---+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.unionByName\n# 根据列名来进行合并数据集\ndf1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\ndf2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\ndf1.unionByName(df2).show()\n# +----+----+----+\n# |col0|col1|col2|\n# +----+----+----+\n# |   1|   2|   3|\n# |   6|   4|   5|\n# +----+----+----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:56:21.016656Z","iopub.execute_input":"2023-10-17T06:56:21.016984Z","iopub.status.idle":"2023-10-17T06:56:21.659086Z","shell.execute_reply.started":"2023-10-17T06:56:21.016964Z","shell.execute_reply":"2023-10-17T06:56:21.657678Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"+----+----+----+\n|col0|col1|col2|\n+----+----+----+\n|   1|   2|   3|\n|   6|   4|   5|\n+----+----+----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=4>***3. DataFrame的列操作APIs***<font>","metadata":{}},{"cell_type":"markdown","source":"这里主要针对的是列进行操作，比如说重命名、排序、空值判断、类型判断等，这里就不展开写demo了，看看语法应该大家都懂了。","metadata":{}},{"cell_type":"code","source":"Column.alias(*alias, **kwargs)  # 重命名列名\nColumn.asc()  # 按照列进行升序排序\nColumn.desc()  # 按照列进行降序排序\nColumn.astype(dataType)  # 类型转换\nColumn.cast(dataType)  # 强制转换类型\nColumn.between(lowerBound, upperBound)  # 返回布尔值，是否在指定区间范围内\nColumn.contains(other)  # 是否包含某个关键词\nColumn.endswith(other)  # 以什么结束的值，如 df.filter(df.name.endswith('ice')).collect()\nColumn.isNotNull()  # 筛选非空的行\nColumn.isNull()\nColumn.isin(*cols)  # 返回包含某些值的行 df[df.name.isin(\"Bob\", \"Mike\")].collect()\nColumn.like(other)  # 返回含有关键词的行\nColumn.when(condition, value)  # 给True的赋值\nColumn.otherwise(value)  # 与when搭配使用，df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\nColumn.rlike(other)  # 可以使用正则的匹配 df.filter(df.name.rlike('ice$')).collect()\nColumn.startswith(other)  # df.filter(df.name.startswith('Al')).collect()\nColumn.substr(startPos, length)  # df.select(df.name.substr(1, 3).alias(\"col\")).collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font size=4>***4. DataFrame的一些思路变换操作APIs***<font>","metadata":{}},{"cell_type":"code","source":"# DataFrame.createOrReplaceGlobalTempView\n# DataFrame.dropGlobalTempView\n# 创建全局的试图，注册后可以使用sql语句来进行操作，生命周期取决于Spark application本身\ndf.createOrReplaceGlobalTempView(\"people\")\nspark.sql(\"select * from global_temp.people where sex = 'M' \").show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |  Sam| 28|   88|  M|\n# |Peter| 55|  100|  M|\n# +-----+---+-----+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:57:59.108284Z","iopub.execute_input":"2023-10-17T06:57:59.108659Z","iopub.status.idle":"2023-10-17T06:57:59.695777Z","shell.execute_reply.started":"2023-10-17T06:57:59.108632Z","shell.execute_reply":"2023-10-17T06:57:59.695028Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|  Sam| 28|   88|  M|\n|Peter| 55|  100|  M|\n+-----+---+-----+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.createOrReplaceTempView\n# DataFrame.dropTempView\n# 创建本地临时试图，生命周期取决于用来创建此数据集的SparkSession\ndf.createOrReplaceTempView(\"tmp_people\")\nspark.sql(\"select * from tmp_people where sex = 'F' \").show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |Flora| 28|   90|  F|\n# |  Mei| 54|   95|  F|\n# +-----+---+-----+---+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:58:33.856488Z","iopub.execute_input":"2023-10-17T06:58:33.856856Z","iopub.status.idle":"2023-10-17T06:58:34.166720Z","shell.execute_reply.started":"2023-10-17T06:58:33.856825Z","shell.execute_reply":"2023-10-17T06:58:34.165801Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|Flora| 28|   90|  F|\n|  Mei| 54|   95|  F|\n+-----+---+-----+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.cache\\DataFrame.persist\n# 可以把一些数据放入缓存中，default storage level (MEMORY_AND_DISK).\ndf.cache()\ndf.persist()\ndf.unpersist()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:58:43.185158Z","iopub.execute_input":"2023-10-17T06:58:43.185521Z","iopub.status.idle":"2023-10-17T06:58:43.203707Z","shell.execute_reply.started":"2023-10-17T06:58:43.185494Z","shell.execute_reply":"2023-10-17T06:58:43.202856Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"23/10/17 06:58:43 WARN CacheManager: Asked to cache already cached data.\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"DataFrame[name: string, age: bigint, score: bigint, sex: string]"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.crossJoin\n# 返回两个DataFrame的笛卡尔积关联的DataFrame\ndf1 = df.select(\"name\", \"sex\")\ndf2 = df.select(\"name\", \"sex\")\ndf3 = df1.crossJoin(df2)\nprint(\"表1的记录数\", df1.count())\nprint(\"表2的记录数\", df2.count())\nprint(\"笛卡尔积后的记录数\", df3.count())\n# 表1的记录数 5\n# 表2的记录数 5\n# 笛卡尔积后的记录数 25","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:58:59.447661Z","iopub.execute_input":"2023-10-17T06:58:59.448022Z","iopub.status.idle":"2023-10-17T06:59:01.539327Z","shell.execute_reply.started":"2023-10-17T06:58:59.447995Z","shell.execute_reply":"2023-10-17T06:59:01.538529Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"表1的记录数 5\n表2的记录数 5\n","output_type":"stream"},{"name":"stderr","text":"[Stage 123:=========================================>             (12 + 4) / 16]\r","output_type":"stream"},{"name":"stdout","text":"笛卡尔积后的记录数 25\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.toPandas\n# 把SparkDataFrame转为 Pandas的DataFrame\ndf.toPandas()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:59:12.223047Z","iopub.execute_input":"2023-10-17T06:59:12.223419Z","iopub.status.idle":"2023-10-17T06:59:12.435558Z","shell.execute_reply.started":"2023-10-17T06:59:12.223391Z","shell.execute_reply":"2023-10-17T06:59:12.434779Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"    name  age  score   sex\n0    Sam   28     88     M\n1  Flora   28     90     F\n2    Run    1     60  None\n3  Peter   55    100     M\n4    Mei   54     95     F","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>age</th>\n      <th>score</th>\n      <th>sex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sam</td>\n      <td>28</td>\n      <td>88</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Flora</td>\n      <td>28</td>\n      <td>90</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run</td>\n      <td>1</td>\n      <td>60</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Peter</td>\n      <td>55</td>\n      <td>100</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Mei</td>\n      <td>54</td>\n      <td>95</td>\n      <td>F</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.rdd\n# 把SparkDataFrame转为rdd，这样子可以用rdd的语法来操作数据\ndf.rdd","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:59:19.181389Z","iopub.execute_input":"2023-10-17T06:59:19.181751Z","iopub.status.idle":"2023-10-17T06:59:19.189114Z","shell.execute_reply.started":"2023-10-17T06:59:19.181724Z","shell.execute_reply":"2023-10-17T06:59:19.188082Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"MapPartitionsRDD[230] at javaToPython at NativeMethodAccessorImpl.java:0"},"metadata":{}}]},{"cell_type":"markdown","source":"<font size=4>***5. DataFrame的一些统计操作APIs***<font>","metadata":{}},{"cell_type":"code","source":"df.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:59:49.212103Z","iopub.execute_input":"2023-10-17T06:59:49.212460Z","iopub.status.idle":"2023-10-17T06:59:49.518468Z","shell.execute_reply.started":"2023-10-17T06:59:49.212432Z","shell.execute_reply":"2023-10-17T06:59:49.517624Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"+-----+---+-----+----+\n| name|age|score| sex|\n+-----+---+-----+----+\n|  Sam| 28|   88|   M|\n|Flora| 28|   90|   F|\n|  Run|  1|   60|NULL|\n|Peter| 55|  100|   M|\n|  Mei| 54|   95|   F|\n+-----+---+-----+----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# DataFrame.cov\n# 计算指定两列的样本协方差\ndf.cov(\"age\", \"score\")\n# 324.59999999999997","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:59:44.008009Z","iopub.execute_input":"2023-10-17T06:59:44.008445Z","iopub.status.idle":"2023-10-17T06:59:44.337285Z","shell.execute_reply.started":"2023-10-17T06:59:44.008411Z","shell.execute_reply":"2023-10-17T06:59:44.335809Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"324.6000000000001"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.corr\n# 计算指定两列的相关系数，DataFrame.corr(col1, col2, method=None)，目前method只支持Pearson相关系数\ndf.corr(\"age\", \"score\", method=\"pearson\")\n# 0.9319004030498815","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:59:58.475214Z","iopub.execute_input":"2023-10-17T06:59:58.475537Z","iopub.status.idle":"2023-10-17T06:59:58.981437Z","shell.execute_reply.started":"2023-10-17T06:59:58.475515Z","shell.execute_reply":"2023-10-17T06:59:58.980685Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"0.9319004030498816"},"metadata":{}}]},{"cell_type":"code","source":"# DataFrame.cube\n# 创建多维度聚合的结果，通常用于分析数据，比如我们指定两个列进行聚合，比如name和age，那么这个函数返回的聚合结果会\n# groupby(\"name\", \"age\")\n# groupby(\"name\")\n# groupby(\"age\")\n# groupby(all)\n# 四个聚合结果的union all 的结果","metadata":{"execution":{"iopub.status.busy":"2023-10-17T07:00:06.253708Z","iopub.execute_input":"2023-10-17T07:00:06.254006Z","iopub.status.idle":"2023-10-17T07:00:06.258465Z","shell.execute_reply.started":"2023-10-17T07:00:06.253987Z","shell.execute_reply":"2023-10-17T07:00:06.257314Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"df1 = df.filter(df.name != \"Run\")\nprint(df1.show())\ndf1.cube(\"name\", \"sex\").count().show()\n# +-----+---+-----+---+\n# | name|age|score|sex|\n# +-----+---+-----+---+\n# |  Sam| 28|   88|  M|\n# |Flora| 28|   90|  F|\n# |Peter| 55|  100|  M|\n# |  Mei| 54|   95|  F|\n# +-----+---+-----+---+\n# cube 聚合之后的结果\n# +-----+----+-----+\n# | name| sex|count|\n# +-----+----+-----+\n# | null|   F|    2|\n# | null|null|    4|\n# |Flora|null|    1|\n# |Peter|null|    1|\n# | null|   M|    2|\n# |Peter|   M|    1|\n# |  Sam|   M|    1|\n# |  Sam|null|    1|\n# |  Mei|   F|    1|\n# |  Mei|null|    1|\n# |Flora|   F|    1|\n# +-----+----+-----+","metadata":{"execution":{"iopub.status.busy":"2023-10-17T07:00:15.450111Z","iopub.execute_input":"2023-10-17T07:00:15.450446Z","iopub.status.idle":"2023-10-17T07:00:16.346811Z","shell.execute_reply.started":"2023-10-17T07:00:15.450419Z","shell.execute_reply":"2023-10-17T07:00:16.346050Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"+-----+---+-----+---+\n| name|age|score|sex|\n+-----+---+-----+---+\n|  Sam| 28|   88|  M|\n|Flora| 28|   90|  F|\n|Peter| 55|  100|  M|\n|  Mei| 54|   95|  F|\n+-----+---+-----+---+\n\nNone\n+-----+----+-----+\n| name| sex|count|\n+-----+----+-----+\n|  Sam|   M|    1|\n|  Sam|NULL|    1|\n| NULL|NULL|    4|\n| NULL|   M|    2|\n| NULL|   F|    2|\n|Flora|   F|    1|\n|Flora|NULL|    1|\n|  Mei|   F|    1|\n|Peter|   M|    1|\n|Peter|NULL|    1|\n|  Mei|NULL|    1|\n+-----+----+-----+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<font size=7><font color=\"orange\">***保存数据/写入数据库***<font>","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:38:56.299544Z","iopub.execute_input":"2023-10-17T06:38:56.299983Z","iopub.status.idle":"2023-10-17T06:38:56.308537Z","shell.execute_reply.started":"2023-10-17T06:38:56.299946Z","shell.execute_reply":"2023-10-17T06:38:56.306298Z"}}},{"cell_type":"markdown","source":"这里的保存数据主要是保存到Hive中的栗子，主要包括了overwrite、append等方式。","metadata":{}},{"cell_type":"markdown","source":"<font size=4>***1. 当结果集为SparkDataFrame的时候***<font>","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder \\\n    .appName(\"sam_SamShare\") \\\n    .config(\"master\", \"local[4]\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n# sc = spark.sparkContext","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:01:52.745433Z","iopub.execute_input":"2023-10-17T09:01:52.745758Z","iopub.status.idle":"2023-10-17T09:01:58.361180Z","shell.execute_reply.started":"2023-10-17T09:01:52.745735Z","shell.execute_reply":"2023-10-17T09:01:58.360246Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"Warning: Ignoring non-Spark config property: master\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/17 09:01:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import HiveContext\n\nconf = SparkConf()\\\n      .setAppName(\"test\")\\\n      .set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") # 动态写入hive分区表\nsc = SparkContext(conf=conf)\nhc = HiveContext(sc)\nsc.setLogLevel(\"ERROR\")\n    \nlist_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]]\nSpark_df = spark.createDataFrame(list_values, ['name', 'age', 'score'])\nprint(Spark_df.show())\nsave_table = \"tmp.samshare_pyspark_savedata\"\n\n# 方式1:直接写入到Hive\nSpark_df.write.format(\"hive\").mode(\"overwrite\").saveAsTable(save_table) # 或者改成append模式\nprint(datetime.now().strftime(\"%y/%m/%d %H:%M:%S\"), \"测试数据写入到表\" + save_table)\n\n# 方式2:注册为临时表，使用SparkSQL来写入分区表\nSpark_df.createOrReplaceTempView(\"tmp_table\")\nwrite_sql = \"\"\"\ninsert overwrite table {0} partitions (pt_date='{1}')\nselect * from tmp_table\n\"\"\".format(save_table, \"20210520\")\nhc.sql(write_sql)\nprint(datetime.now().strftime(\"%y/%m/%d %H:%M:%S\"), \"测试数据写入到表\" + save_table)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T09:02:12.494073Z","iopub.execute_input":"2023-10-17T09:02:12.494490Z","iopub.status.idle":"2023-10-17T09:02:12.546023Z","shell.execute_reply.started":"2023-10-17T09:02:12.494458Z","shell.execute_reply":"2023-10-17T09:02:12.544295Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HiveContext\n\u001b[1;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\\\n\u001b[1;32m      8\u001b[0m       \u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m       \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive.exec.dynamic.partition.mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 动态写入hive分区表\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m hc \u001b[38;5;241m=\u001b[39m HiveContext(sc)\n\u001b[1;32m     12\u001b[0m sc\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n","\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=sam_SamShare, master=local[*]) created by getOrCreate at /tmp/ipykernel_33/2776350081.py:5 "],"ename":"ValueError","evalue":"Cannot run multiple SparkContexts at once; existing SparkContext(app=sam_SamShare, master=local[*]) created by getOrCreate at /tmp/ipykernel_33/2776350081.py:5 ","output_type":"error"}]},{"cell_type":"markdown","source":"<font size=4>***2. 当结果集为Python的DataFrame的时候***<font>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datetime import datetime\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import HiveContext\n\nconf = SparkConf()\\\n      .setAppName(\"test\")\\\n      .set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") # 动态写入hive分区表\nsc = SparkContext(conf=conf)\nhc = HiveContext(sc)\nsc.setLogLevel(\"ERROR\")\n    \nresult_df = pd.DataFrame([1,2,3], columns=['a'])\nsave_table = \"tmp.samshare_pyspark_savedata\"\n\n# 获取DataFrame的schema\nc1 = list(result_df.columns)\n# 转为SparkDataFrame\nresult = hc.createDataFrame(result_df.astype(str), c1)\nresult.write.format(\"hive\").mode(\"overwrite\").saveAsTable(save_table) # 或者改成append模式\nprint(datetime.now().strftime(\"%y/%m/%d %H:%M:%S\"), \"测试数据写入到表\" + save_table)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T07:02:43.239660Z","iopub.execute_input":"2023-10-17T07:02:43.240048Z","iopub.status.idle":"2023-10-17T07:02:43.280520Z","shell.execute_reply.started":"2023-10-17T07:02:43.240021Z","shell.execute_reply":"2023-10-17T07:02:43.279426Z"},"trusted":true},"execution_count":70,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HiveContext\n\u001b[1;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\\\n\u001b[1;32m      8\u001b[0m       \u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m       \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive.exec.dynamic.partition.mode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# 动态写入hive分区表\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m hc \u001b[38;5;241m=\u001b[39m HiveContext(sc)\n\u001b[1;32m     12\u001b[0m sc\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n","\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=test_SamShare, master=local[4]) created by __init__ at /tmp/ipykernel_32/1162219135.py:2 "],"ename":"ValueError","evalue":"Cannot run multiple SparkContexts at once; existing SparkContext(app=test_SamShare, master=local[4]) created by __init__ at /tmp/ipykernel_32/1162219135.py:2 ","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}