{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <font size=15>**常用函数**<font>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:20:18.704685Z","iopub.execute_input":"2023-10-17T06:20:18.705113Z","iopub.status.idle":"2023-10-17T06:21:06.656060Z","shell.execute_reply.started":"2023-10-17T06:20:18.705080Z","shell.execute_reply":"2023-10-17T06:21:06.654227Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425350 sha256=ed33e5781b3a14120f280bf449b1b264a4134a7ab16a186892ef3d755ea9ac75\n  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pyspark\nfrom pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setAppName(\"test_SamShare\").setMaster(\"local[4]\")\nsc = SparkContext(conf=conf)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:21:06.659102Z","iopub.execute_input":"2023-10-17T06:21:06.659642Z","iopub.status.idle":"2023-10-17T06:21:12.989632Z","shell.execute_reply.started":"2023-10-17T06:21:06.659594Z","shell.execute_reply":"2023-10-17T06:21:12.988635Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/10/17 06:21:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"# 使用 parallelize方法直接实例化一个RDD\nrdd = sc.parallelize(range(1,11),4) # 这里的 4 指的是分区数量\nrdd.take(100)\n# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:22:27.118909Z","iopub.execute_input":"2023-10-17T06:22:27.119424Z","iopub.status.idle":"2023-10-17T06:22:27.747305Z","shell.execute_reply.started":"2023-10-17T06:22:27.119383Z","shell.execute_reply":"2023-10-17T06:22:27.746108Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"},"metadata":{}}]},{"cell_type":"markdown","source":"----------------------------------------------\n                Transform算子解析\n----------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:21:49.276042Z","iopub.execute_input":"2023-10-17T06:21:49.276496Z","iopub.status.idle":"2023-10-17T06:21:49.285669Z","shell.execute_reply.started":"2023-10-17T06:21:49.276461Z","shell.execute_reply":"2023-10-17T06:21:49.284242Z"}}},{"cell_type":"code","source":"# 以下的操作由于是Transform操作，因为我们需要在最后加上一个collect算子用来触发计算。\n# 1. map: 和python差不多，map转换就是对每一个元素进行一个映射\nrdd = sc.parallelize(range(1, 11), 4)\nrdd_map = rdd.map(lambda x: x*2)\nprint(\"原始数据：\", rdd.collect())\nprint(\"扩大2倍：\", rdd_map.collect())\n# 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# 扩大2倍： [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:22:29.675060Z","iopub.execute_input":"2023-10-17T06:22:29.675496Z","iopub.status.idle":"2023-10-17T06:22:30.463650Z","shell.execute_reply.started":"2023-10-17T06:22:29.675464Z","shell.execute_reply":"2023-10-17T06:22:30.462446Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n扩大2倍： [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. flatMap: 这个相比于map多一个flat（压平）操作，顾名思义就是要把高维的数组变成一维\nrdd2 = sc.parallelize([\"hello SamShare\", \"hello PySpark\"])\nprint(\"原始数据：\", rdd2.collect())\nprint(\"直接split之后的map结果：\", rdd2.map(lambda x: x.split(\" \")).collect())\nprint(\"直接split之后的flatMap结果：\", rdd2.flatMap(lambda x: x.split(\" \")).collect())\n# 直接split之后的map结果： [['hello', 'SamShare'], ['hello', 'PySpark']]\n# 直接split之后的flatMap结果： ['hello', 'SamShare', 'hello', 'PySpark']","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:23:08.888672Z","iopub.execute_input":"2023-10-17T06:23:08.889138Z","iopub.status.idle":"2023-10-17T06:23:09.946592Z","shell.execute_reply.started":"2023-10-17T06:23:08.889099Z","shell.execute_reply":"2023-10-17T06:23:09.945179Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"原始数据： ['hello SamShare', 'hello PySpark']\n直接split之后的map结果： [['hello', 'SamShare'], ['hello', 'PySpark']]\n直接split之后的flatMap结果： ['hello', 'SamShare', 'hello', 'PySpark']\n","output_type":"stream"}]},{"cell_type":"code","source":"# 3. filter: 过滤数据\nrdd = sc.parallelize(range(1, 11), 4)\nprint(\"原始数据：\", rdd.collect())\nprint(\"过滤奇数：\", rdd.filter(lambda x: x % 2 == 0).collect())\n# 原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# 过滤奇数： [2, 4, 6, 8, 10]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:23:26.678545Z","iopub.execute_input":"2023-10-17T06:23:26.678935Z","iopub.status.idle":"2023-10-17T06:23:27.448494Z","shell.execute_reply.started":"2023-10-17T06:23:26.678903Z","shell.execute_reply":"2023-10-17T06:23:27.447423Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"原始数据： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n过滤奇数： [2, 4, 6, 8, 10]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 4. distinct: 去重元素\nrdd = sc.parallelize([2, 2, 4, 8, 8, 8, 8, 16, 32, 32])\nprint(\"原始数据：\", rdd.collect())\nprint(\"去重数据：\", rdd.distinct().collect())\n# 原始数据： [2, 2, 4, 8, 8, 8, 8, 16, 32, 32]\n# 去重数据： [4, 8, 16, 32, 2]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:23:38.367462Z","iopub.execute_input":"2023-10-17T06:23:38.367925Z","iopub.status.idle":"2023-10-17T06:23:39.441644Z","shell.execute_reply.started":"2023-10-17T06:23:38.367890Z","shell.execute_reply":"2023-10-17T06:23:39.440499Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"原始数据： [2, 2, 4, 8, 8, 8, 8, 16, 32, 32]\n去重数据： [4, 8, 16, 32, 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 5. reduceByKey: 根据key来映射数据\nfrom operator import add\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nprint(\"原始数据：\", rdd.collect())\nprint(\"原始数据：\", rdd.reduceByKey(add).collect())\n# 原始数据： [('a', 1), ('b', 1), ('a', 1)]\n# 原始数据： [('b', 1), ('a', 2)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:23:52.440418Z","iopub.execute_input":"2023-10-17T06:23:52.440835Z","iopub.status.idle":"2023-10-17T06:23:53.354779Z","shell.execute_reply.started":"2023-10-17T06:23:52.440793Z","shell.execute_reply":"2023-10-17T06:23:53.353611Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"原始数据： [('a', 1), ('b', 1), ('a', 1)]\n原始数据： [('b', 1), ('a', 2)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. mapPartitions: 根据分区内的数据进行映射操作\nrdd = sc.parallelize([1, 2, 3, 4], 2)\ndef f(iterator):\n    yield sum(iterator)\nprint(rdd.collect())\nprint(rdd.mapPartitions(f).collect())\n# [1, 2, 3, 4]\n# [3, 7]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:24:15.586144Z","iopub.execute_input":"2023-10-17T06:24:15.586586Z","iopub.status.idle":"2023-10-17T06:24:15.918825Z","shell.execute_reply.started":"2023-10-17T06:24:15.586552Z","shell.execute_reply":"2023-10-17T06:24:15.917696Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[1, 2, 3, 4]\n[3, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 7. sortBy: 根据规则进行排序\ntmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\nprint(sc.parallelize(tmp).sortBy(lambda x: x[0]).collect())\nprint(sc.parallelize(tmp).sortBy(lambda x: x[1]).collect())\n# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n# [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:24:53.853180Z","iopub.execute_input":"2023-10-17T06:24:53.854715Z","iopub.status.idle":"2023-10-17T06:24:56.971531Z","shell.execute_reply.started":"2023-10-17T06:24:53.854666Z","shell.execute_reply":"2023-10-17T06:24:56.970379Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 8. subtract: 数据集相减, Return each value in self that is not contained in other.\nx = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\ny = sc.parallelize([(\"a\", 3), (\"c\", None)])\nprint(sorted(x.subtract(y).collect()))\n# [('a', 1), ('b', 4), ('b', 5)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:25:44.153553Z","iopub.execute_input":"2023-10-17T06:25:44.153992Z","iopub.status.idle":"2023-10-17T06:25:46.016194Z","shell.execute_reply.started":"2023-10-17T06:25:44.153960Z","shell.execute_reply":"2023-10-17T06:25:46.015008Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"[Stage 38:====================================>                     (5 + 3) / 8]\r","output_type":"stream"},{"name":"stdout","text":"[('a', 1), ('b', 4), ('b', 5)]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# 9. union: 合并两个RDD\nrdd = sc.parallelize([1, 1, 2, 3])\nprint(rdd.union(rdd).collect())\n# [1, 1, 2, 3, 1, 1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:26:17.455802Z","iopub.execute_input":"2023-10-17T06:26:17.456306Z","iopub.status.idle":"2023-10-17T06:26:17.519444Z","shell.execute_reply.started":"2023-10-17T06:26:17.456265Z","shell.execute_reply":"2023-10-17T06:26:17.518370Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[1, 1, 2, 3, 1, 1, 2, 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 10. intersection: 取两个RDD的交集，同时有去重的功效\nrdd1 = sc.parallelize([1, 10, 2, 3, 4, 5, 2, 3])\nrdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\nprint(rdd1.intersection(rdd2).collect())\n# [1, 2, 3]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:26:35.808717Z","iopub.execute_input":"2023-10-17T06:26:35.809125Z","iopub.status.idle":"2023-10-17T06:26:37.599934Z","shell.execute_reply.started":"2023-10-17T06:26:35.809093Z","shell.execute_reply":"2023-10-17T06:26:37.599075Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"[Stage 43:====================================>                     (5 + 3) / 8]\r","output_type":"stream"},{"name":"stdout","text":"[1, 2, 3]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# 11. cartesian: 生成笛卡尔积\nrdd = sc.parallelize([1, 2])\nprint(sorted(rdd.cartesian(rdd).collect()))\n# [(1, 1), (1, 2), (2, 1), (2, 2)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:26:47.115095Z","iopub.execute_input":"2023-10-17T06:26:47.115663Z","iopub.status.idle":"2023-10-17T06:26:47.189911Z","shell.execute_reply.started":"2023-10-17T06:26:47.115622Z","shell.execute_reply":"2023-10-17T06:26:47.188761Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[(1, 1), (1, 2), (2, 1), (2, 2)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 12. zip: 拉链合并，需要两个RDD具有相同的长度以及分区数量\nx = sc.parallelize(range(0, 5))\ny = sc.parallelize(range(1000, 1005))\nprint(x.collect())\nprint(y.collect())\nprint(x.zip(y).collect())\n# [0, 1, 2, 3, 4]\n# [1000, 1001, 1002, 1003, 1004]\n# [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:26:59.687691Z","iopub.execute_input":"2023-10-17T06:26:59.688133Z","iopub.status.idle":"2023-10-17T06:27:01.010789Z","shell.execute_reply.started":"2023-10-17T06:26:59.688101Z","shell.execute_reply":"2023-10-17T06:27:01.009495Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[0, 1, 2, 3, 4]\n[1000, 1001, 1002, 1003, 1004]\n[(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 13. zipWithIndex: 将RDD和一个从0开始的递增序列按照拉链方式连接。\nrdd_name = sc.parallelize([\"LiLei\", \"Hanmeimei\", \"Lily\", \"Lucy\", \"Ann\", \"Dachui\", \"RuHua\"])\nrdd_index = rdd_name.zipWithIndex()\nprint(rdd_index.collect())\n# [('LiLei', 0), ('Hanmeimei', 1), ('Lily', 2), ('Lucy', 3), ('Ann', 4), ('Dachui', 5), ('RuHua', 6)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:27:22.821349Z","iopub.execute_input":"2023-10-17T06:27:22.821858Z","iopub.status.idle":"2023-10-17T06:27:23.434395Z","shell.execute_reply.started":"2023-10-17T06:27:22.821817Z","shell.execute_reply":"2023-10-17T06:27:23.433297Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[('LiLei', 0), ('Hanmeimei', 1), ('Lily', 2), ('Lucy', 3), ('Ann', 4), ('Dachui', 5), ('RuHua', 6)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 14. groupByKey: 按照key来聚合数据\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nprint(rdd.collect())\nprint(sorted(rdd.groupByKey().mapValues(len).collect()))\nprint(sorted(rdd.groupByKey().mapValues(list).collect()))\n# [('a', 1), ('b', 1), ('a', 1)]\n# [('a', 2), ('b', 1)]\n# [('a', [1, 1]), ('b', [1])]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:27:55.026378Z","iopub.execute_input":"2023-10-17T06:27:55.026825Z","iopub.status.idle":"2023-10-17T06:27:56.913545Z","shell.execute_reply.started":"2023-10-17T06:27:55.026788Z","shell.execute_reply":"2023-10-17T06:27:56.912514Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[('a', 1), ('b', 1), ('a', 1)]\n[('a', 2), ('b', 1)]\n[('a', [1, 1]), ('b', [1])]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 15. sortByKey:\ntmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\nprint(sc.parallelize(tmp).sortByKey(True, 1).collect())\n# [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:28:23.255459Z","iopub.execute_input":"2023-10-17T06:28:23.255883Z","iopub.status.idle":"2023-10-17T06:28:23.450699Z","shell.execute_reply.started":"2023-10-17T06:28:23.255851Z","shell.execute_reply":"2023-10-17T06:28:23.449436Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 16. join:\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)])\ny = sc.parallelize([(\"a\", 2), (\"a\", 3)])\nprint(sorted(x.join(y).collect()))\n# [('a', (1, 2)), ('a', (1, 3))]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:28:47.773598Z","iopub.execute_input":"2023-10-17T06:28:47.774003Z","iopub.status.idle":"2023-10-17T06:28:49.594016Z","shell.execute_reply.started":"2023-10-17T06:28:47.773972Z","shell.execute_reply":"2023-10-17T06:28:49.592651Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"[Stage 59:====================================>                     (5 + 3) / 8]\r","output_type":"stream"},{"name":"stdout","text":"[('a', (1, 2)), ('a', (1, 3))]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"# 17. leftOuterJoin/rightOuterJoin\nx = sc.parallelize([(\"a\", 1), (\"b\", 4)])\ny = sc.parallelize([(\"a\", 2)])\nprint(sorted(x.leftOuterJoin(y).collect()))\n# [('a', (1, 2)), ('b', (4, None))]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:28:58.815631Z","iopub.execute_input":"2023-10-17T06:28:58.815983Z","iopub.status.idle":"2023-10-17T06:29:01.053363Z","shell.execute_reply.started":"2023-10-17T06:28:58.815955Z","shell.execute_reply":"2023-10-17T06:29:01.052141Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"[Stage 61:==================================================>       (7 + 1) / 8]\r","output_type":"stream"},{"name":"stdout","text":"[('a', (1, 2)), ('b', (4, None))]\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"----------------------------------------------\n                Action算子解析\n----------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:29:38.090807Z","iopub.execute_input":"2023-10-17T06:29:38.091167Z","iopub.status.idle":"2023-10-17T06:29:38.098765Z","shell.execute_reply.started":"2023-10-17T06:29:38.091138Z","shell.execute_reply":"2023-10-17T06:29:38.097153Z"}}},{"cell_type":"code","source":"# 1. collect: 指的是把数据都汇集到driver端，便于后续的操作\nrdd = sc.parallelize(range(0, 5))\nrdd_collect = rdd.collect()\nprint(rdd_collect)\n# [0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:29:48.395940Z","iopub.execute_input":"2023-10-17T06:29:48.396482Z","iopub.status.idle":"2023-10-17T06:29:48.803203Z","shell.execute_reply.started":"2023-10-17T06:29:48.396431Z","shell.execute_reply":"2023-10-17T06:29:48.802054Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"[0, 1, 2, 3, 4]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. first: 取第一个元素\nsc.parallelize([2, 3, 4]).first()\n# 2","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:29:56.611551Z","iopub.execute_input":"2023-10-17T06:29:56.612017Z","iopub.status.idle":"2023-10-17T06:29:56.992349Z","shell.execute_reply.started":"2023-10-17T06:29:56.611982Z","shell.execute_reply":"2023-10-17T06:29:56.991554Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"# 3. collectAsMap: 转换为dict，使用这个要注意了，不要对大数据用，不然全部载入到driver端会爆内存\nm = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\nm\n# {1: 2, 3: 4}","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:30:06.843431Z","iopub.execute_input":"2023-10-17T06:30:06.843820Z","iopub.status.idle":"2023-10-17T06:30:06.882825Z","shell.execute_reply.started":"2023-10-17T06:30:06.843789Z","shell.execute_reply":"2023-10-17T06:30:06.882019Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{1: 2, 3: 4}"},"metadata":{}}]},{"cell_type":"code","source":"# 4. reduce: 逐步对两个元素进行操作\nrdd = sc.parallelize(range(10),5)\nprint(rdd.reduce(lambda x,y:x+y))\n# 45","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:30:25.835444Z","iopub.execute_input":"2023-10-17T06:30:25.835847Z","iopub.status.idle":"2023-10-17T06:30:26.244342Z","shell.execute_reply.started":"2023-10-17T06:30:25.835815Z","shell.execute_reply":"2023-10-17T06:30:26.243279Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"45\n","output_type":"stream"}]},{"cell_type":"code","source":"# 5. countByKey/countByValue:\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nprint(sorted(rdd.countByKey().items()))\nprint(sorted(rdd.countByValue().items()))\n# [('a', 2), ('b', 1)]\n# [(('a', 1), 2), (('b', 1), 1)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:30:39.438041Z","iopub.execute_input":"2023-10-17T06:30:39.438537Z","iopub.status.idle":"2023-10-17T06:30:40.093177Z","shell.execute_reply.started":"2023-10-17T06:30:39.438499Z","shell.execute_reply":"2023-10-17T06:30:40.092022Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"[('a', 2), ('b', 1)]\n[(('a', 1), 2), (('b', 1), 1)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. take: 相当于取几个数据到driver端\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nprint(rdd.take(5))\n# [('a', 1), ('b', 1), ('a', 1)]","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:31:09.512847Z","iopub.execute_input":"2023-10-17T06:31:09.513206Z","iopub.status.idle":"2023-10-17T06:31:09.953479Z","shell.execute_reply.started":"2023-10-17T06:31:09.513176Z","shell.execute_reply":"2023-10-17T06:31:09.952164Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"[('a', 1), ('b', 1), ('a', 1)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 7. saveAsTextFile: 保存rdd成text文件到本地\ntext_file = \"./data/rdd.txt\"\nrdd = sc.parallelize(range(5))\nrdd.saveAsTextFile(text_file)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:31:26.743004Z","iopub.execute_input":"2023-10-17T06:31:26.743380Z","iopub.status.idle":"2023-10-17T06:31:27.545356Z","shell.execute_reply.started":"2023-10-17T06:31:26.743324Z","shell.execute_reply":"2023-10-17T06:31:27.544087Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# 8. takeSample: 随机取数\nrdd = sc.textFile(\"./data/rdd.txt\", 4)  # 这里的 4 指的是分区数量\nrdd_sample = rdd.takeSample(True, 2, 0)  # withReplacement 参数1：代表是否是有放回抽样\nrdd_sample","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:32:34.447308Z","iopub.execute_input":"2023-10-17T06:32:34.447787Z","iopub.status.idle":"2023-10-17T06:32:35.701257Z","shell.execute_reply.started":"2023-10-17T06:32:34.447755Z","shell.execute_reply":"2023-10-17T06:32:35.700159Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"['1', '0']"},"metadata":{}}]},{"cell_type":"code","source":"# 9. foreach: 对每一个元素执行某种操作，不生成新的RDD\nrdd = sc.parallelize(range(10), 5)\naccum = sc.accumulator(0)\nrdd.foreach(lambda x: accum.add(x))\nprint(accum.value)\n# 45","metadata":{"execution":{"iopub.status.busy":"2023-10-17T06:32:42.548292Z","iopub.execute_input":"2023-10-17T06:32:42.548795Z","iopub.status.idle":"2023-10-17T06:32:43.004534Z","shell.execute_reply.started":"2023-10-17T06:32:42.548760Z","shell.execute_reply":"2023-10-17T06:32:43.003391Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"45\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}